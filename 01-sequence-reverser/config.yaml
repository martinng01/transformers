model:
  d_model: 128
  num_heads: 4
  num_layers: 4
  d_ff: 512

training:
  batch_size: 32
  lr: 0.001
  epochs: 5
  vocab_size: 99
  seq_len: 10
